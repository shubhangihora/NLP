{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aim to predict whether a movie review is positive or negative using the text in the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#CountVectorizer is a function that converts text into a term - document matrix\n",
    "#this function breaks down text into individual terms and stores the occurence of the term in a piece of text as an element of a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very little music or anything to speak of.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best scene in the movie was when Gerardo i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  A very, very, very slow-moving, aimless movie ...      0\n",
       "1  Not sure who was more lost - the flat characte...      0\n",
       "2  Attempting artiness with black & white and cle...      0\n",
       "3       Very little music or anything to speak of.        0\n",
       "4  The best scene in the movie was when Gerardo i...      1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"imdb_labelled.txt\"     #reading the file\n",
    "names = ['text', 'label']       #determining the columns we want\n",
    "df = pd.read_csv(file, header = None, names = names, sep = '\\t', quoting = 3)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000x3047 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 12666 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = CountVectorizer()\n",
    "x = bow.fit_transform(df.text)     #converting the text document into a term - document matrix and storing it in x\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1639)\t1\n",
      "  (0, 3037)\t1\n",
      "  (0, 786)\t1\n",
      "  (0, 748)\t1\n",
      "  (0, 37)\t1\n",
      "  (0, 1748)\t1\n",
      "  (0, 92)\t1\n",
      "  (0, 1750)\t1\n",
      "  (0, 2404)\t1\n",
      "  (0, 2871)\t3\n",
      "  (1, 1875)\t1\n",
      "  (1, 2905)\t1\n",
      "  (1, 2969)\t1\n",
      "  (1, 1837)\t1\n",
      "  (1, 1206)\t1\n",
      "  (1, 1777)\t1\n",
      "  (1, 196)\t1\n",
      "  (1, 1862)\t1\n",
      "  (1, 431)\t1\n",
      "  (1, 1035)\t1\n",
      "  (1, 2638)\t2\n",
      "  (1, 1605)\t1\n",
      "  (1, 1733)\t1\n",
      "  (1, 2917)\t1\n",
      "  (1, 2965)\t1\n",
      "  :\t:\n",
      "  (996, 1852)\t1\n",
      "  (996, 2658)\t1\n",
      "  (996, 1358)\t1\n",
      "  (996, 1605)\t1\n",
      "  (996, 2917)\t1\n",
      "  (997, 837)\t1\n",
      "  (997, 3001)\t1\n",
      "  (997, 1428)\t1\n",
      "  (997, 1423)\t1\n",
      "  (997, 1358)\t1\n",
      "  (998, 911)\t1\n",
      "  (998, 222)\t1\n",
      "  (999, 1393)\t1\n",
      "  (999, 1397)\t1\n",
      "  (999, 1316)\t1\n",
      "  (999, 1430)\t1\n",
      "  (999, 1725)\t1\n",
      "  (999, 2921)\t1\n",
      "  (999, 123)\t1\n",
      "  (999, 1854)\t1\n",
      "  (999, 100)\t2\n",
      "  (999, 1358)\t1\n",
      "  (999, 2694)\t1\n",
      "  (999, 125)\t1\n",
      "  (999, 1837)\t1\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now we must find those words that are useful\n",
    "\n",
    "for example - the words \"it\", \"is\", \"the\", \"a\" are not useful but the words \"loved\", \"great\", \"hated\", \"awful\" are.\n",
    "we do this by assigning weights to each word that appears\n",
    "\n",
    "the basic weight formula would be -- number of times word appears in text / proportion of texts word appears in\n",
    "\n",
    "so if \"loved\" appears in the first text once and appears in 1% of all the texts, then its weight would be -- 1 / 1% = 100\n",
    "\n",
    "and if \"movie\" appears in the first text once and appears in 33% of all the texts, then its weight would be -- 1 / 33% = 3\n",
    "\n",
    "but this process might give rare / misspelled words a very high weightage, which wont benefit us and so we need to downweight super frequent words without overweighting rare ones - which is what <b> term frequency - document inverse frequency </b> does\n",
    "\n",
    "the <b> tf-idf formula</b> is -- number of times word appears in text / log( 1 / proportion of texts word appears in)\n",
    "\n",
    "the elements in bow represent the (tf) term freq. aka the number of times word appears in text, so now we need to find the idf \n",
    "\n",
    "scikit learn is super kind and has a function much like CountVectorizer that finds the tf-idf matrix for us, called TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "x = tfidf.fit_transform(df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2871)\t0.538231616282\n",
      "  (0, 2404)\t0.279231657722\n",
      "  (0, 1750)\t0.294988181032\n",
      "  (0, 92)\t0.337896791919\n",
      "  (0, 1748)\t0.129034479447\n",
      "  (0, 37)\t0.191065991859\n",
      "  (0, 748)\t0.337896791919\n",
      "  (0, 786)\t0.337896791919\n",
      "  (0, 3037)\t0.294988181032\n",
      "  (0, 1639)\t0.250242919112\n",
      "  (1, 1813)\t0.16837950966\n",
      "  (1, 2567)\t0.298871784424\n",
      "  (1, 2965)\t0.195713228608\n",
      "  (1, 2917)\t0.128708711135\n",
      "  (1, 1733)\t0.208098455651\n",
      "  (1, 1605)\t0.288646914192\n",
      "  (1, 2638)\t0.153084697784\n",
      "  (1, 1035)\t0.312053934844\n",
      "  (1, 431)\t0.198190693531\n",
      "  (1, 1862)\t0.192231407001\n",
      "  (1, 196)\t0.28029258691\n",
      "  (1, 1777)\t0.330633132357\n",
      "  (1, 1206)\t0.273229103839\n",
      "  (1, 1837)\t0.0998301966822\n",
      "  (1, 2969)\t0.312053934844\n",
      "  :\t:\n",
      "  (996, 3003)\t0.269069466257\n",
      "  (996, 2288)\t0.261415181915\n",
      "  (996, 2812)\t0.344992074668\n",
      "  (996, 2886)\t0.395174189216\n",
      "  (996, 2143)\t0.395174189216\n",
      "  (997, 1358)\t0.271928281324\n",
      "  (997, 1423)\t0.224838839198\n",
      "  (997, 1428)\t0.232666392035\n",
      "  (997, 3001)\t0.619969808539\n",
      "  (997, 837)\t0.661064514796\n",
      "  (998, 222)\t0.46735005602\n",
      "  (998, 911)\t0.884072352886\n",
      "  (999, 1837)\t0.123146420323\n",
      "  (999, 125)\t0.118228184245\n",
      "  (999, 2694)\t0.142165196378\n",
      "  (999, 1358)\t0.151654504479\n",
      "  (999, 100)\t0.431211885008\n",
      "  (999, 1854)\t0.203751687148\n",
      "  (999, 123)\t0.222617901552\n",
      "  (999, 2921)\t0.293964924707\n",
      "  (999, 1725)\t0.337044172845\n",
      "  (999, 1430)\t0.265090987768\n",
      "  (999, 1316)\t0.356062948901\n",
      "  (999, 1397)\t0.356062948901\n",
      "  (999, 1393)\t0.356062948901\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is a massive matrix, since there are so many words and there are two problems when working with massive matrices:\n",
    "- they take up too much comp memory\n",
    "- theyre harder to train models on to predict outcomes from new, unseen data\n",
    "\n",
    "so, we can use a sparse matrix, which stores only the non-zero elements. CountVectorizer() and TfidfVectorizer() return sparse matrices\n",
    "\n",
    "to deal with problem number two, we can use logistic regression which filters out the columns that are not necessary for classification - technique called regularization <b> OR </b> we can compress the term - document matrix\n",
    "\n",
    "compressing is like clustering - each word is assigned a score based on how closely it is associated with a cluster. for example - \"bad\", \"awful\", \"terrible\" all have similar meanings, and so by this method they'll be compressed into one column\n",
    "\n",
    "^^ known as reducing the dimensionality of a term - document matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.7809999999999999\n",
      "Pipeline(memory=None,\n",
      "     steps=[('tfidfvectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_i...rflow.python.training.adam.AdamOptimizer'>,\n",
      "       solver_kwargs=None, transform_layer_index=None))])\n"
     ]
    }
   ],
   "source": [
    "from muffnn import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Chain together tf-idf and an MLP with a single hidden layer of size 256\n",
    "mlp = MLPClassifier(hidden_units=(256,))\n",
    "classifier = make_pipeline(tfidf, mlp)\n",
    "\n",
    "# Get cross-validated accuracy of the model\n",
    "cv_accuracy = cross_val_score(classifier, df.text, df.label, cv=5)\n",
    "print(\"Mean Accuracy: {}\".format(np.mean(cv_accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
